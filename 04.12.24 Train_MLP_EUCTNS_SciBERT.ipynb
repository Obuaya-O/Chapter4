{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing classificiation of primary endpoint type in datasets using MLP with SciBERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np # Don't think I need this but it's just habit at this point\n",
    "import re\n",
    "import string\n",
    "\n",
    "#import torch\n",
    "#from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s2421127\\AppData\\Local\\miniconda3\\envs\\clustering_endpoints\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "euct_ns = pd.read_csv('c:\\\\Users\\\\s2421127\\\\Documents\\\\NLP Project\\\\ObuayaO\\\\NLP project\\\\Chapter 3\\\\euct_ns.csv', encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns = ['Title', 'Objective', 'pr_endpoint', 'endpoint_description']\n",
    "X = euct_ns[text_columns] \n",
    "y = euct_ns['manual_label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is words so needs to be converted into numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[text_columns].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s2421127\\AppData\\Local\\miniconda3\\envs\\clustering_endpoints\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained SciBERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text and generate embeddings\n",
    "def generate_embeddings(texts, tokenizer, model, max_len=512):\n",
    "    \"\"\"Generate embeddings for a list of texts using SciBERT.\"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(\n",
    "            texts.tolist(), \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=max_len, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        outputs = model(**inputs)\n",
    "        # Use the [CLS] token representation (typically at index 0)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for the dataset\n",
    "X_embeddings = generate_embeddings(X, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embeddings.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(X_embeddings, \"embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_embeddings, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "joblib.dump(scaler, \"scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_embeddings during scaler training: (152, 768)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train_embeddings during scaler training:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an MLPClassifier model\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(64, 32),\n",
    "                    max_iter=1000, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.79%\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the training data\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(mlp, \"model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred) # There is no cases of intermediate outcomes in the pred set. Do I re-run it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply MLP model to NS-HRA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_hra = pd.read_csv('c:\\\\Users\\\\s2421127\\\\Documents\\\\NLP Project\\\\ObuayaO\\\\NLP project\\\\Chapter 3\\\\ns_hra.csv', encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this should be properly cleaned and I can re-run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = joblib.load('embeddings.pkl')\n",
    "scaler = joblib.load('scaler_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns = ['Title', 'Objective', '1ry_endpoint'] # In the HRA REC forms, the primary endpoint and endpoint description are together\n",
    "X2 = ns_hra[text_columns] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = X2[text_columns].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MLPClassifier' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X2_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m, in \u001b[0;36mgenerate_embeddings\u001b[1;34m(texts, tokenizer, model, max_len)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_embeddings\u001b[39m(texts, tokenizer, model, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m):\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate embeddings for a list of texts using SciBERT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      6\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[0;32m      7\u001b[0m             texts\u001b[38;5;241m.\u001b[39mtolist(), \n\u001b[0;32m      8\u001b[0m             padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m             return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m         )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MLPClassifier' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "X2_embeddings = generate_embeddings(X2, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = X2_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = scaler.transform(X2_embeddings) # I did this in the training so I guess I have to do that here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.predict(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_scores = mlp.predict_proba(X2) # How sure is the model on the predictions that it made?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euct_ns_pred = pd.DataFrame(confidence_scores, columns=['PFO_0', 'IO_1', 'SO_2'])\n",
    "euct_ns_pred['Predicted_label'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(euct_ns_pred.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euct_ns_pred.to_csv('euct_ns_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive statistics\n",
    "# Need to know what the hell is going on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data visualisation of clusters - t-SNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix to dense\n",
    "X2_dense = X2.toarray()\n",
    "\n",
    "# Reduce dimensions using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X2_tsne = tsne.fit_transform(X2_dense)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X2_tsne[:, 0], X2_tsne[:, 1], c=y_pred, cmap='viridis', marker='o', edgecolor='k')\n",
    "plt.title('t-SNE of predicted primary endpoint types in the NS-HRA dataset')\n",
    "plt.colorbar(scatter)\n",
    "plt.grid(True)\n",
    "#plt.savefig('t-SNE of predicted primary endpoint types in the NS-HRA dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slope chart for frequency distribution across different labels\n",
    "pfo_df = ns_hra[y_pred == 0]\n",
    "io_df = ns_hra[y_pred == 1]\n",
    "so_df = ns_hra[y_pred == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfo_df.head() #Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfo_df = pfo_df.copy()\n",
    "io_df = io_df.copy()\n",
    "so_df = so_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfo_df['concat_corpus'] = pfo_df['Title']+ \" \" + pfo_df['Objective'] + \" \" + pfo_df['1ry_endpoint'] \n",
    "io_df['concat_corpus'] = io_df['Title']+ \" \" + io_df['Objective'] + \" \" + io_df['1ry_endpoint'] \n",
    "so_df['concat_corpus'] = so_df['Title']+ \" \" + so_df['Objective'] + \" \" +so_df['1ry_endpoint'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfo_df.head() #sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with an empty string\n",
    "so_df['concat_corpus'] = so_df['concat_corpus'].fillna('')\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(so_df['concat_corpus'])\n",
    "so_tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(so_tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_tfidf_scores = np.asarray(tfidf_matrix.mean(axis=0)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_tfidf_df_list = pd.DataFrame({'word': feature_names, 'tfidf_score': so_tfidf_scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_tfidf = so_tfidf_df_list.sort_values(by='tfidf_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " top_n = 618"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_tfidf.head(top_n).to_csv('tf-idf so.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfo_tfidf_df = pd.read_csv('c:\\\\Users\\\\s2421127\\\\Documents\\\\NLP Project\\\\ObuayaO\\\\NLP project\\\\Chapter 3\\\\tf-idf pfo.csv', encoding='unicode_escape')\n",
    "io_tfidf_df = pd.read_csv('c:\\\\Users\\\\s2421127\\\\Documents\\\\NLP Project\\\\ObuayaO\\\\NLP project\\\\Chapter 3\\\\tf-idf io.csv', encoding='unicode_escape')\n",
    "so_tfidf_df = pd.read_csv('c:\\\\Users\\\\s2421127\\\\Documents\\\\NLP Project\\\\ObuayaO\\\\NLP project\\\\Chapter 3\\\\tf-idf so.csv', encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pfo_tfidf_df.merge(io_tfidf_df, on='word', suffixes=('_pfo', '_io'))\n",
    "merged_df = merged_df.merge(so_tfidf_df, on='word')\n",
    "merged_df.rename(columns={'tfidf_score': 'tfidf_score_so'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()\n",
    "# The first one is clearly a problem with the structure of XML files and i've cleaned as best as I can google. I'll just delete row 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.iloc[1:]\n",
    "merged_df = merged_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['max_diff'] = merged_df[['tfidf_score_pfo', 'tfidf_score_io', 'tfidf_score_so']].max(axis=1) - \\\n",
    "                        merged_df[['tfidf_score_pfo', 'tfidf_score_io', 'tfidf_score_so']].min(axis=1)\n",
    "\n",
    "sorted_df = merged_df.sort_values(by='max_diff', ascending=False)\n",
    "\n",
    "top_features_df = sorted_df.head(15)\n",
    "print(top_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features_df = top_features_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "jitter_strength = 0.4\n",
    "\n",
    "for i in range(len(top_features_df)):\n",
    "    x_positions = np.array([0, 1, 2]) + np.random.normal(0, jitter_strength, size=3)\n",
    "    ax.plot(['pfo', 'io', 'so'], \n",
    "            [top_features_df.iloc[i]['tfidf_score_pfo'], top_features_df.iloc[i]['tfidf_score_io'], top_features_df.iloc[i]['tfidf_score_so']],\n",
    "            marker='o', label=top_features_df['word'].iloc[i])\n",
    "\n",
    "for i, word in enumerate(top_features_df['word']):\n",
    "    for j, subset in enumerate(['pfo', 'io', 'so']):\n",
    "        ax.text(j, top_features_df.iloc[i, j+1], f'{word}', verticalalignment='center', fontsize=8)\n",
    "\n",
    "ax.axvline(x=1, color='gray', linestyle='--', linewidth=1) \n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Slope Chart of Largest Change in TF-IDF Score Across Predicted Primary Endpoint Label in The NS-HRA dataset', fontsize=16)\n",
    "ax.set_ylabel('TF-IDF Score')\n",
    "ax.set_xticks(['pfo', 'io', 'so'])\n",
    "ax.grid(True, which='both', axis='y', linestyle='--', linewidth=0.7)\n",
    "\n",
    "# Display the slope chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix of TF-IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concordance_df = merged_df[['word', 'tfidf_score_pfo', 'tfidf_score_io', 'tfidf_score_so']]\n",
    "correlation_matrix = concordance_df[['tfidf_score_pfo', 'tfidf_score_io', 'tfidf_score_so']].corr()\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='Greens', square=True)\n",
    "\n",
    "plt.title('Correlation Matrix of TF-IDF Scores Across Predicted Labels in NS-HRA dataset', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clustering_endpoints",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
