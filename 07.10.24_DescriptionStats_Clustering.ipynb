{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import bigrams, trigrams\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"c:\\\\Users\\\\s2421127\\\\Documents\\\\NLP Project\\\\ObuayaO\\\\NLP project\\\\02.09.24_ns.csv\", encoding = 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EudraCT_No</th>\n",
       "      <th>Title</th>\n",
       "      <th>Phase</th>\n",
       "      <th>Objective</th>\n",
       "      <th>End_date</th>\n",
       "      <th>Sample_size</th>\n",
       "      <th>pr_endpoint</th>\n",
       "      <th>endpoint_description</th>\n",
       "      <th>Treatment</th>\n",
       "      <th>LT_followup</th>\n",
       "      <th>manual_label</th>\n",
       "      <th>Reasoning</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>pr_bigrams</th>\n",
       "      <th>pr_trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-003243-39</td>\n",
       "      <td>A Phase 3, Randomized, Double-Blind, Placebo-C...</td>\n",
       "      <td>3</td>\n",
       "      <td>The primary purpose of this study is to evalua...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>175</td>\n",
       "      <td>Primary: Change From Baseline In Myasthenia Gr...</td>\n",
       "      <td>MG-ADL: 8-point questionnaire focusing on rele...</td>\n",
       "      <td>Ravulizumab</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Specific to disease HRQoL questionnaire</td>\n",
       "      <td>activating notch</td>\n",
       "      <td>activating notch mutations</td>\n",
       "      <td>amifampridine phosphate</td>\n",
       "      <td>exam digitally video</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-016138-29</td>\n",
       "      <td>âRandomized, Multicenter, Open-label, Phase ...</td>\n",
       "      <td>iii</td>\n",
       "      <td>To compare the efficacy of plitidepsin in comb...</td>\n",
       "      <td>20-Nov-17</td>\n",
       "      <td>255</td>\n",
       "      <td>Primary: Progression-free Survival (Independen...</td>\n",
       "      <td>The primary study analysis was based on extern...</td>\n",
       "      <td>Aplidin</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Standard surrogate outcome</td>\n",
       "      <td>angle resolution</td>\n",
       "      <td>bleed hepatic encephalopathy</td>\n",
       "      <td>consciousness awareness</td>\n",
       "      <td>delayed cerebral ischemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-000474-38</td>\n",
       "      <td>A Multicenter, 2-Cohort Trial to First Assess ...</td>\n",
       "      <td>0</td>\n",
       "      <td>To demonstrate that fenfluramine hydrochloride...</td>\n",
       "      <td>05-Jun-18</td>\n",
       "      <td>87</td>\n",
       "      <td>Primary: Change in Convulsive Seizure Frequenc...</td>\n",
       "      <td>Baseline-adjusted in CSF (mean number of convu...</td>\n",
       "      <td>fenfluramine hydrochloride</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Development of disease</td>\n",
       "      <td>anticoagulant apixaban</td>\n",
       "      <td>boundary shift integral</td>\n",
       "      <td>cover orofacial</td>\n",
       "      <td>microglia activation via</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-000418-75</td>\n",
       "      <td>A Multicenter, Multinational, Randomized, Doub...</td>\n",
       "      <td>0</td>\n",
       "      <td>The primary objective of this study was to ass...</td>\n",
       "      <td>19-Jun-18</td>\n",
       "      <td>352</td>\n",
       "      <td>Primary: Change From Baseline in UHDRS-TMS at ...</td>\n",
       "      <td>UHDRS assess motor function, cognition, behavi...</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Specific to disease HRQoL questionnaire</td>\n",
       "      <td>bacterial peritonitis</td>\n",
       "      <td>exam digitally video</td>\n",
       "      <td>deep inspiration</td>\n",
       "      <td>positron emission tomography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-002933-12</td>\n",
       "      <td>A Phase II pilot study to explore treatment wi...</td>\n",
       "      <td>ii</td>\n",
       "      <td>To determine whether patients taking a medicin...</td>\n",
       "      <td>10-Dec-18</td>\n",
       "      <td>8</td>\n",
       "      <td>Primary: Workload',</td>\n",
       "      <td>All participants cycled on a cycle ergometer. ...</td>\n",
       "      <td>Sodium Valproate</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>Endpoint on function</td>\n",
       "      <td>biliary cholangitis</td>\n",
       "      <td>meld uncontrolled ascites</td>\n",
       "      <td>detect follow</td>\n",
       "      <td>well extremity truncal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       EudraCT_No                                              Title Phase  \\\n",
       "0  2018-003243-39  A Phase 3, Randomized, Double-Blind, Placebo-C...     3   \n",
       "1  2009-016138-29  âRandomized, Multicenter, Open-label, Phase ...   iii   \n",
       "2  2016-000474-38  A Multicenter, 2-Cohort Trial to First Assess ...     0   \n",
       "3  2014-000418-75  A Multicenter, Multinational, Randomized, Doub...     0   \n",
       "4  2012-002933-12  A Phase II pilot study to explore treatment wi...    ii   \n",
       "\n",
       "                                           Objective   End_date  Sample_size  \\\n",
       "0  The primary purpose of this study is to evalua...        NaN          175   \n",
       "1  To compare the efficacy of plitidepsin in comb...  20-Nov-17          255   \n",
       "2  To demonstrate that fenfluramine hydrochloride...  05-Jun-18           87   \n",
       "3  The primary objective of this study was to ass...  19-Jun-18          352   \n",
       "4  To determine whether patients taking a medicin...  10-Dec-18            8   \n",
       "\n",
       "                                         pr_endpoint  \\\n",
       "0  Primary: Change From Baseline In Myasthenia Gr...   \n",
       "1  Primary: Progression-free Survival (Independen...   \n",
       "2  Primary: Change in Convulsive Seizure Frequenc...   \n",
       "3  Primary: Change From Baseline in UHDRS-TMS at ...   \n",
       "4                                Primary: Workload',   \n",
       "\n",
       "                                endpoint_description  \\\n",
       "0  MG-ADL: 8-point questionnaire focusing on rele...   \n",
       "1  The primary study analysis was based on extern...   \n",
       "2  Baseline-adjusted in CSF (mean number of convu...   \n",
       "3  UHDRS assess motor function, cognition, behavi...   \n",
       "4  All participants cycled on a cycle ergometer. ...   \n",
       "\n",
       "                    Treatment LT_followup  manual_label  \\\n",
       "0                 Ravulizumab          No             0   \n",
       "1                     Aplidin          No             2   \n",
       "2  fenfluramine hydrochloride         Yes             0   \n",
       "3                     Placebo          No             0   \n",
       "4            Sodium Valproate          No             1   \n",
       "\n",
       "                                 Reasoning                 bigrams  \\\n",
       "0  Specific to disease HRQoL questionnaire        activating notch   \n",
       "1              Standard surrogate outcome         angle resolution   \n",
       "2                   Development of disease  anticoagulant apixaban   \n",
       "3  Specific to disease HRQoL questionnaire   bacterial peritonitis   \n",
       "4                     Endpoint on function     biliary cholangitis   \n",
       "\n",
       "                       trigrams               pr_bigrams  \\\n",
       "0    activating notch mutations  amifampridine phosphate   \n",
       "1  bleed hepatic encephalopathy  consciousness awareness   \n",
       "2       boundary shift integral          cover orofacial   \n",
       "3          exam digitally video         deep inspiration   \n",
       "4     meld uncontrolled ascites            detect follow   \n",
       "\n",
       "                    pr_trigrams  \n",
       "0          exam digitally video  \n",
       "1     delayed cerebral ischemia  \n",
       "2      microglia activation via  \n",
       "3  positron emission tomography  \n",
       "4        well extremity truncal  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slope plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfo_df = df[df['manual_label'] == 0]\n",
    "io_df = df[df['manual_label']== 1]\n",
    "so_df = df[df['manual_label']== 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_freq(x):\n",
    "    freqs = [(value, x.count(value) / len(x)) for value in set(x)] \n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s2421127\\AppData\\Local\\Temp\\ipykernel_15680\\1739584776.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pfo_df['corpus'] = pfo_df['pr_endpoint'] + \" \" + pfo_df['endpoint_description']\n"
     ]
    }
   ],
   "source": [
    "pfo_df['corpus'] = pfo_df['pr_endpoint'] + \" \" + pfo_df['endpoint_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      3\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m----> 4\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpfo_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcorpus\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# primary endpoint text and concatenated text\u001b[39;00m\n\u001b[0;32m      5\u001b[0m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n",
      "File \u001b[1;32mc:\\Users\\s2421127\\AppData\\Local\\miniconda3\\envs\\clustering_endpoints\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2091\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2084\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2085\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2086\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2087\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2088\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2089\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2090\u001b[0m )\n\u001b[1;32m-> 2091\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2093\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2094\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\s2421127\\AppData\\Local\\miniconda3\\envs\\clustering_endpoints\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\s2421127\\AppData\\Local\\miniconda3\\envs\\clustering_endpoints\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\s2421127\\AppData\\Local\\miniconda3\\envs\\clustering_endpoints\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1259\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1258\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1260\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1261\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\s2421127\\AppData\\Local\\miniconda3\\envs\\clustering_endpoints\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:103\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 103\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m     doc \u001b[38;5;241m=\u001b[39m analyzer(doc)\n",
      "File \u001b[1;32mc:\\Users\\s2421127\\AppData\\Local\\miniconda3\\envs\\clustering_endpoints\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:236\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    233\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_error)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan:\n\u001b[1;32m--> 236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m     )\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "# tf_idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(pfo_df['corpus']) # primary endpoint text and concatenated text\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['collocates'] = df['bigrams'] + \" \" + df['trigrams'] # I think this is okay but nothing else\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2, 3)) \n",
    "X_collocates = vectorizer.fit_transform(df['collocates'])\n",
    "\n",
    "y_labels = df['manual_label']\n",
    "\n",
    "mi_scores = mutual_info_classif(X_collocates, manual_labels)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "mi_scores_df = pd.DataFrame({'Collocate': feature_names, 'MI Score': mi_scores})\n",
    "\n",
    "mi_scores_df = mi_scores_df.sort_values(by='MI Score', ascending=False)\n",
    "print(mi_scores_df.head(20))  # Show top 10 most informative bigrams/trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bigrams - primary text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_collocates = vectorizer.fit_transform(df['pr_bigrams']) # This cannot be right\n",
    "\n",
    "y_labels = df['manual_label']\n",
    "\n",
    "mi_scores = mutual_info_classif(X_collocates, y_labels)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "mi_scores_df = pd.DataFrame({'Collocate': feature_names, 'MI Score': mi_scores})\n",
    "\n",
    "mi_scores_df = mi_scores_df.sort_values(by='MI Score', ascending=False)\n",
    "print(mi_scores_df.head(20))  # Show top 10 most informative bigrams/trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slope Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate relative frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a list of collocates that change the most \n",
    "collocates = [] # Do one for unigram, bigrams, trigrams\n",
    "\n",
    "#plot the chart\n",
    "fig, ax = plt.subplots()\n",
    "for i in collocates:\n",
    "    temp = df[df['country_origin'] == i]\n",
    "    plt.plot(temp.year, temp.refugees_number, color='#0072BC', marker='o', markersize=5)\n",
    "    # start label\n",
    "    plt.text(temp.year.values[0]-0.4, temp.refugees_number.values[0], i, ha='right', va='center')\n",
    "    # end label\n",
    "    plt.text(temp.year.values[1]+0.4, temp.refugees_number.values[1], i, ha='left', va='center')\n",
    "    \n",
    "ticks = plt.xticks([2000, 2021])\n",
    "xl = plt.xlim(1990,2031)\n",
    "yl = plt.ylim(0, 7*1e6)\n",
    "\n",
    "#set chart title\n",
    "ax.set_title('Evolution of refugee population by country of origin |2000 vs 2021')\n",
    "\n",
    "#set y-axis label\n",
    "ax.set_ylabel('Number of people (millions)')\n",
    "\n",
    "#format x-axis tick labels\n",
    "def number_formatter(x, pos):\n",
    "    if x >= 1e6:\n",
    "        s = '{:1.0f}M'.format(x*1e-6)\n",
    "    elif x < 1e6 and x > 0:\n",
    "        s = '{:1.0f}K'.format(x*1e-3)\n",
    "    else: \n",
    "        s = '{:1.0f}'.format(x)\n",
    "    return s\n",
    "ax.yaxis.set_major_formatter(number_formatter)\n",
    "\n",
    "#set chart source and copyright\n",
    "plt.annotate('Source: UNHCR Refugee Data Finder', (0,0), (0, -25), xycoords='axes fraction', textcoords='offset points', va='top', color = '#666666', fontsize=9)\n",
    "plt.annotate('©UNHCR, The UN Refugee Agency', (0,0), (0, -35), xycoords='axes fraction', textcoords='offset points', va='top', color = '#666666', fontsize=9)\n",
    "\n",
    "#adjust chart margin and layout\n",
    "fig.tight_layout()\n",
    "\n",
    "#show chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import parallel_coordinates\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collocate_features = pd.DataFrame(X_collocates.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "collocate_features['label'] = df['manual_label']\n",
    "\n",
    "subset = collocate_features.sample(20, axis=1) \n",
    "subset['label'] = df['manual_label']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "parallel_coordinates(subset, class_column='label', colormap=plt.get_cmap('Set3'))\n",
    "\n",
    "plt.title(\"Parallel Coordinate Plot for Primary Text Bigram Collocates and Manual Labels\")\n",
    "plt.ylabel(\"Feature Value (TF-IDF Score)\")\n",
    "plt.xticks(rotation=45, fontsize=6, fontweight='bold')\n",
    "plt.xlabel(\"Collocate Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collocate_features = pd.DataFrame(X_collocates.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "collocate_features['label'] = df['manual_label']\n",
    "\n",
    "subset = collocate_features.sample(20, axis=1) \n",
    "subset['label'] = df['manual_label']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "parallel_coordinates(subset, class_column='label', colormap=plt.get_cmap('Set3'))\n",
    "\n",
    "# Step 6: Display the plot\n",
    "plt.title(\"Parallel Coordinate Plot for Collocates and Manual Labels\")\n",
    "plt.ylabel(\"Feature Value (TF-IDF Score)\")\n",
    "plt.xticks(rotation=45, fontsize=8, fontweight='bold')\n",
    "plt.xlabel(\"Collocate Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collocates = vectorizer.inverse_transform(X_collocates)\n",
    "\n",
    "cfdist = ConditionalFreqDist()\n",
    "\n",
    "for collocate_list, label in zip(collocates, df['manual_label']):  \n",
    "    collocate_str = ' '.join(collocate_list)  \n",
    "    tokens = word_tokenize(collocate_str) \n",
    "    \n",
    "    \n",
    "    for word in tokens:\n",
    "        cfdist[label][word] += 1\n",
    "\n",
    "print(cfdist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collocates = vectorizer.inverse_transform(X_collocates)\n",
    "\n",
    "cfdist = ConditionalFreqDist()\n",
    "\n",
    "for collocate_list, label in zip(collocates, df['manual_label']):  \n",
    "    collocate_str = ' '.join(collocate_list)  \n",
    "    tokens = word_tokenize(collocate_str) \n",
    "    \n",
    "    \n",
    "    for ngram in tokens:\n",
    "        cfdist[label][ngram] += 1\n",
    "\n",
    "print(cfdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in df['manual_label'].unique():\n",
    "    print(f\"\\nMost common collocates for label {label}:\")\n",
    "    print(cfdist[label].most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_cfdist = ConditionalFreqDist()\n",
    "trigram_cfdist = ConditionalFreqDist()\n",
    "\n",
    "for collocate_list, label in zip(collocates, df['manual_label']): \n",
    "    collocate_str = ' '.join(collocate_list)  \n",
    "    tokens = word_tokenize(collocate_str)  \n",
    "\n",
    "    \n",
    "    bigram_list = list(bigrams(tokens))   \n",
    "    trigram_list = list(trigrams(tokens))  \n",
    "    \n",
    "    for bigram in bigram_list:\n",
    "        bigram_cfdist[label][bigram] += 1 \n",
    "\n",
    "    for trigram in trigram_list:\n",
    "        trigram_cfdist[label][trigram] += 1  \n",
    "\n",
    "\n",
    "print(\"Most common bigrams by label:\")\n",
    "for label in df['manual_label'].unique():\n",
    "    print(f\"\\nLabel {label}:\")\n",
    "    print(bigram_cfdist[label].most_common(10))  \n",
    "\n",
    "print(\"\\nMost common trigrams by label:\")\n",
    "for label in df['manual_label'].unique():\n",
    "    print(f\"\\nLabel {label}:\")\n",
    "    print(trigram_cfdist[label].most_common(10))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primary endpoint text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_collocation_analysis(file_path, text_columns): \n",
    "    \"\"\"Perform collocation analysis on a given text file to find top bigrams and trigrams, excluding those that occur only once.\"\"\"\n",
    "    df = pd.read_csv(\"c:\\\\Users\\\\s2421127\\\\Documents\\\\NLP Project\\\\ObuayaO\\\\NLP project\\\\02.09.24_ns.csv\", encoding = 'unicode_escape')\n",
    "    text = \" \".join(df[text_columns].astype(str).apply(lambda row: ' '.join(row), axis=1).tolist())\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    words = [word.lower() for word in tokens if word.isalpha()]\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    trigram_measures = TrigramAssocMeasures()\n",
    "    bigram_finder = BigramCollocationFinder.from_words(filtered_words)\n",
    "    trigram_finder = TrigramCollocationFinder.from_words(filtered_words)\n",
    "\n",
    "    bigram_finder.apply_freq_filter(2)\n",
    "    trigram_finder.apply_freq_filter(2)\n",
    "\n",
    "    top_bigrams = bigram_finder.nbest(bigram_measures.pmi, 190) # Matched with how many rows I have\n",
    "    top_trigrams = trigram_finder.nbest(trigram_measures.pmi, 190)\n",
    "\n",
    "    return top_bigrams, top_trigrams\n",
    "\n",
    "top_bigrams, top_trigrams = perform_collocation_analysis(\"c:\\\\Users\\\\s2421127\\\\Documents\\\\NLP Project\\\\ObuayaO\\\\NLP project\\\\02.09.24_ns.csv\", [\"pr_endpoint\", \"endpoint_description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"collocation-results_pr.txt\", \"w\", encoding='utf-8') as output_file:\n",
    "    output_file.write(\"Top Bigram Collocations (excluding those occurring only once):\\n\")\n",
    "    for bigram in top_bigrams:\n",
    "        output_file.write(f\"{bigram[0]} {bigram[1]}\\n\")\n",
    "    output_file.write(\"\\nTop Trigram Collocations (excluding those occurring only once):\\n\")\n",
    "    for trigram in top_trigrams:\n",
    "        output_file.write(f\"{trigram[0]} {trigram[1]} {trigram[2]}\\n\")\n",
    "\n",
    "print(\"Collocation analysis is complete. Results are saved in 'collocation-results_pr.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurrence(text, window_size=2):\n",
    "    # Load stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Tokenize the text and filter out stopwords\n",
    "    tokens = [token.lower() for token in word_tokenize(text) if token.isalpha() and token.lower() not in stop_words]\n",
    "    \n",
    "    # Initialize co-occurrence count\n",
    "    co_occurrence_counts = Counter()\n",
    "    \n",
    "    # Calculate co-occurrences within the specified window size\n",
    "    for i in range(len(tokens)):\n",
    "        token = tokens[i]\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(tokens), i + window_size + 1)\n",
    "        for j in range(start, end):\n",
    "            if i != j:\n",
    "                co_occurred_token = tokens[j]\n",
    "                co_occurrence_counts[(token, co_occurred_token)] += 1\n",
    "    \n",
    "    # Return the top 10 most common co-occurrences\n",
    "    return co_occurrence_counts.most_common(10)\n",
    "\n",
    "# Read the text file\n",
    "with open(\"collocation-results_pr.txt\", \"r\", encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Calculate the top 10 co-occurrences\n",
    "top_co_occurrences = co_occurrence(text, window_size=2)\n",
    "\n",
    "# Output the results\n",
    "with open(\"top-co-occurrence-results_pr.txt\", \"w\", encoding='utf-8') as output_file:\n",
    "    for pair, freq in top_co_occurrences:\n",
    "        output_file.write(f\"{pair[0]}, {pair[1]}: {freq}\\n\")\n",
    "\n",
    "print(\"Top 10 co-occurrence analysis is complete. Results are saved in 'top-co-occurrence-primary-results_190.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['collocates'] = df['pr_bigrams'] + \" \" + df['pr_trigrams']\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2, 3)) \n",
    "X_collocates = vectorizer.fit_transform(df['collocates'])\n",
    "\n",
    "y_labels = df['manual_label']\n",
    "\n",
    "mi_scores = mutual_info_classif(X_collocates, manual_labels)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "mi_scores_df = pd.DataFrame({'Collocate': feature_names, 'MI Score': mi_scores})\n",
    "\n",
    "mi_scores_df = mi_scores_df.sort_values(by='MI Score', ascending=False)\n",
    "print(mi_scores_df.head(20))  # Show top 10 most informative bigrams/trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collocate_features = pd.DataFrame(X_collocates.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "collocate_features['label'] = df['manual_label']\n",
    "\n",
    "subset = collocate_features.sample(20, axis=1) \n",
    "subset['label'] = df['manual_label']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "parallel_coordinates(subset, class_column='label', colormap=plt.get_cmap('Set2'))\n",
    "\n",
    "# Step 6: Display the plot\n",
    "plt.title(\"Parallel Coordinate Plot for Primary Text Collocates and Manual Labels\")\n",
    "plt.ylabel(\"Feature Value (TF-IDF Score)\")\n",
    "plt.xticks(rotation=45, fontsize=6, fontweight='bold')\n",
    "plt.xlabel(\"Collocate Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "collocates = vectorizer.inverse_transform(X_collocates)\n",
    "\n",
    "cfdist = ConditionalFreqDist()\n",
    "\n",
    "for collocate_list, label in zip(collocates, df['manual_label']):  \n",
    "    collocate_str = ' '.join(collocate_list)  \n",
    "    tokens = word_tokenize(collocate_str) \n",
    "    \n",
    "    \n",
    "    for word in tokens:\n",
    "        cfdist[label][word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_cfdist = ConditionalFreqDist()\n",
    "trigram_cfdist = ConditionalFreqDist()\n",
    "\n",
    "for collocate_list, label in zip(collocates, df['manual_label']): \n",
    "    collocate_str = ' '.join(collocate_list)  \n",
    "    tokens = word_tokenize(collocate_str)  \n",
    "\n",
    "    \n",
    "    bigram_list = list(bigrams(tokens))   \n",
    "    trigram_list = list(trigrams(tokens))  \n",
    "    \n",
    "    for bigram in bigram_list:\n",
    "        bigram_cfdist[label][bigram] += 1 \n",
    "\n",
    "    for trigram in trigram_list:\n",
    "        trigram_cfdist[label][trigram] += 1  \n",
    "\n",
    "\n",
    "print(\"Most common bigrams by label:\")\n",
    "for label in df['manual_label'].unique():\n",
    "    print(f\"\\nLabel {label}:\")\n",
    "    print(bigram_cfdist[label].most_common(10))  \n",
    "\n",
    "print(\"\\nMost common trigrams by label:\")\n",
    "for label in df['manual_label'].unique():\n",
    "    print(f\"\\nLabel {label}:\")\n",
    "    print(trigram_cfdist[label].most_common(10))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOOCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove all non-word characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = text.lower()  # Convert to lower case\n",
    "    text = text.split()  # Split into words\n",
    "    text = [lemmatizer.lemmatize(word) for word in text if word not in stopwords.words('english')]  # Lemmatize and remove stopwords # Get rid of primary as a stop word\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def strip_accents(text):\n",
    "    return ''. join(word for word in unicodedata.normalize ('NFD', text)\n",
    "                     if unicodedata.category(word) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title'] = df['Title'].apply(preprocess_text)\n",
    "df['Objective'] = df['Objective'].apply(preprocess_text)\n",
    "df['pr_endpoint'] = df['pr_endpoint'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['endpoint_description'] = df['endpoint_description'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['endpoint_description'] = df['endpoint_description'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title'] = df['Title'].apply(strip_accents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns = ['Title', 'Objective', 'pr_endpoint', 'endpoint_description', 'LT_followup']\n",
    "X = df[text_columns] # What are the predictors of primary endpoint type? I do not know if the n-grams should be included here as they are not 'natural' observations [ 'bigrams', 'trigrams', 'pr_bigrams', 'pr_trigrams']\n",
    "y = df['manual_label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined = X[text_columns].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(preprocessor=preprocess_text, strip_accents='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_tfidf = tfidf.fit_transform(X_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might now be obsolete\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('tfidf', tfidf, text_columns)  # Apply TF-IDF to the text columns\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might delete\n",
    "pipeline = make_pipeline(preprocessor, RandomForestRegressor(n_estimators = 50, random_state = 4)) # 50 to 500 is the standard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators = 50, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# Set the number of folds equal to the number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, make_scorer # MAE is better for interpretability \n",
    "mae_scorer = make_scorer(mean_absolute_error)\n",
    "#import recall, precision, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_val_score(\n",
    "    estimator=rfc, # Changed from pipeline\n",
    "    X=X_tfidf, # changed from X to X_tfidf\n",
    "    y=y,\n",
    "    cv=n,  \n",
    "    scoring=mae_scorer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_rows = X[text_columns].apply(lambda x: x.str.strip()).eq('').any(axis=1)\n",
    "print(f\"Rows with empty or all-whitespace text: {empty_rows.sum()}\")\n",
    "print(df[empty_rows])  # Print the rows with empty or whitespace text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_results)\n",
    "print('The mean: {}'.format(cv_results.mean()))\n",
    "print('The std: {}'.format(cv_results.std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clustering_endpoints",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
